# -*- coding: utf-8 -*-
"""analise_preditiva.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B5p4vcsvxmvvS-5q68pfhPiAorewVJuc
"""

# importar pacote que permite ler e processar tabelas
import pandas as pd

# ler dados
df = pd.read_csv('https://raw.githubusercontent.com/alvesclamonteiro/datasets/main/notas_analise_preditiva.csv')

# visualizar dados
df.head()

# Analisar as variáveis preditoras

# Analisar as variáveis preditoras

# 1. Tipos de variáveis

# Identificar o tipo de cada variável
df.dtypes

# 2. Estatísticas descritivas

# Calcular estatísticas descritivas para cada variável preditora
df.describe()

# 3. Distribuição das variáveis

# Plotar histogramas para cada variável preditora
df.hist()

# 4. Correlação entre as variáveis preditoras

# Calcular a matriz de correlação entre as variáveis preditoras
df.corr()

# 5. Análise de outliers

# Identificar outliers em cada variável preditora
import seaborn as sns
for col in df.columns:
    sns.boxplot(x=df[col])

# 6. Conclusão

# Com base na análise das variáveis preditoras, podemos concluir que:

# - As variáveis preditoras são todas numéricas.
# - As variáveis preditoras têm distribuições diferentes.
# - Algumas variáveis preditoras apresentam outliers.
# - As variáveis preditoras estão correlacionadas entre si.

df

# visualizar tamanho da base
df.shape

# Analisar os dados como tabela

from google.colab import data_table

data_table.DataTable(df, include_index=True)

"""### Preprocessamento"""

# Analisar só a coluna diabetes

df['diabetes'].value_counts()

# separando os dados em features - variaveis preditoras (X) e target - o alvo a prever (y)
X = df[['sexo', 'nao_lanchou', 'se_preparou', 'nivel_educacao_pais']]
y = df['nota_final']

# importar pacotes que permite dividir os dados de treino e teste de forma aleatória
from sklearn.model_selection import train_test_split

# dividindo os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""## Treinando Modelos de Regressão

#### Treinando um Modelo de Regressão Linear
"""

# importar funcao do modelo de regressao linear
from sklearn.linear_model import LinearRegression

# criando e treinando o modelo de regressão linear
linear_regression = LinearRegression()
linear_regression.fit(X_train, y_train)

## visualizando metricas de performance ##

# importar metricas de performance para regressao
from sklearn.metrics import mean_absolute_error

# Fazendo previsões no conjunto de teste
y_pred = linear_regression.predict(X_test)

# Calculando o erro médio absoluto (MAE)
mae = mean_absolute_error(y_test, y_pred)
print("Erro médio absoluto (MAE):", mae)

# visualizando os coeficientes
coefficients = pd.DataFrame({'Coeficiente': linear_regression.coef_, 'Variável': X.columns})
print(coefficients)

coefficients.to_csv('coefficients.csv', index=False)

"""#### Treinando um Modelo XGboost Regressor"""

# importar pacote xgb
import xgboost as xgb

# criando e treinando o modelo de regressão XGBoost
xgboost_regressor = xgb.XGBRegressor()
xgboost_regressor.fit(X_train, y_train)

# fazendo previsões no conjunto de teste
y_pred = xgboost_regressor.predict(X_test)

## visualizando metricas de performance ##

# importar metricas de performance para regressao
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# calculando o erro médio absoluto (MAE)
mae = mean_absolute_error(y_test, y_pred)
print("Erro médio absoluto (MAE):", mae)

# Visualizando a importância das características
xgb.plot_importance(xgboost_regressor)

print(xgboost_regressor.feature_importances_)

"""## Treinando Modelos de Classificação"""

# ler base de dados
df = pd.read_csv('https://raw.githubusercontent.com/alvesclamonteiro/datasets/main/diabetes.csv')

# visualizar distribuicao do target
df.diabetes.value_counts()

# separar variaveis do alvo
X = df[['gravidez', 'glucose', 'pressao_sanguinea', 'grossura_da_pele', 'nivel_insulina', 'BMI', 'idade']]
y = df['diabetes']

print(X)

# Analisar os dados como tabela
data_table.DataTable(df, include_index=True)

# Analise sexo 1

df.groupby('sexo')['nota_final'].describe()

# dividir em treino e teste
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.25)

"""### Treinando uma Árvore de Decisão"""

# importar arvore de decisao
from sklearn.tree import DecisionTreeClassifier

# criar e treinar modelo
dt = DecisionTreeClassifier()
dt.fit(X_train, Y_train)

# realizar previsoes na base de teste
y_pred = dt.predict(X_test)

# importar metricas de avaliacao de modelos de classificacao
from sklearn.metrics import f1_score, precision_score, recall_score

# Recall
print('Recall: ', recall_score(Y_test, y_pred))

# Precision
print('Precision: ', precision_score(Y_test, y_pred))

# F1-Score
print('F1 Score: ', f1_score(Y_test, y_pred))

# visualizar importancia das variaveis
important_features  = pd.DataFrame({'Variável': X_test.columns, 'Importância': dt.feature_importances_}).sort_values(by='Importância', ascending=False)
important_features



# salvar
important_features.to_csv('important_features_arvore.csv', index=False)

"""#### Treinando um XGBoost Classifier"""

# train XGBoost classifier
xgb =  xgb.XGBClassifier()
xgb.fit(X_train, Y_train)

# prever casos na base de teste
y_pred = xgb.predict(X_test)

print(y_pred)

# Recall
print('Recall: ', recall_score(Y_test, y_pred))

# Precision
print('Precision: ', precision_score(Y_test, y_pred))

# F1-Score
print('F1 Score: ', f1_score(Y_test, y_pred))

# visualizar importancia das variaveis
important_features  = pd.DataFrame({'Variável': X_test.columns, 'Importância': xgb.feature_importances_}).sort_values(by='Importância', ascending=False)
important_features

# salvar
important_features.to_csv('important_features_xgb.csv', index=False)

import matplotlib.pyplot as plt

# Dados de importância das características dos modelos de regressão
features = ['sexo', 'nao_lanchou', 'se_preparou', 'nivel_educacao_pais']
importance_linear = [3.7524, -9.0696, 8.0214, 2.2395]
importance_xgboost = [0.0711, 0.4776, 0.3606, 0.0906]

fig, ax = plt.subplots(figsize=(8, 5))
bar_width = 0.35
index = range(len(features))

bar1 = plt.bar(index, importance_linear, bar_width, label='Linear Regression')
bar2 = plt.bar([p + bar_width for p in index], importance_xgboost, bar_width, label='XGBoost Regressor')

plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importance in Regression Models')
plt.xticks([p + bar_width / 2 for p in index], features)
plt.legend()

plt.tight_layout()
plt.show()

# Dados de importância das características dos modelos de classificação
features_dt = ['gravidez', 'glucose', 'pressao_sanguinea', 'grossura_da_pele', 'nivel_insulina', 'BMI', 'idade']
importance_dt = [0.0278, 0.3424, 0.1269, 0.0388, 0.0607, 0.2145, 0.1860]
importance_xgb = [0.1095, 0.3035, 0.0792, 0.0524, 0.0755, 0.2603, 0.1196]

fig, ax = plt.subplots(figsize=(10, 6))
bar_width = 0.35
index = range(len(features_dt))

bar1 = plt.bar(index, importance_dt, bar_width, label='Decision Tree')
bar2 = plt.bar([p + bar_width for p in index], importance_xgb, bar_width, label='XGBoost Classifier')

plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importance in Classification Models')
plt.xticks([p + bar_width / 2 for p in index], features_dt)
plt.legend()

plt.tight_layout

()
plt.show()

# Analisar Decision Tree

# Dados de importância das características dos modelos de classificação
features_dt = ['gravidez', 'glucose', 'pressao_sanguinea', 'grossura_da_pele', 'nivel_insulina', 'BMI', 'idade']
importance_dt = [0.0278, 0.3424, 0.1269, 0.0388, 0.0607, 0.2145, 0.1860]

fig, ax = plt.subplots(figsize=(10, 6))
bar_width = 0.35
index = range(len(features_dt))

bar1 = plt.bar(index, importance_dt, bar_width, label='Decision Tree')

plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importance in Classification Models - Decision Tree')
plt.xticks([p + bar_width / 2 for p in index], features_dt)
plt.legend()

plt.tight_layout

()
plt.show()

# Analisar XGBoost Classifier

# Dados de importância das características dos modelos de classificação
features_xgb = ['gravidez', 'glucose', 'pressao_sanguinea', 'grossura_da_pele', 'nivel_insulina', 'BMI', 'idade']
importance_xgb = [0.1095, 0.3035, 0.0792, 0.0524, 0.0755, 0.2603, 0.1196]

fig, ax = plt.subplots(figsize=(10, 6))
bar_width = 0.35
index = range(len(features_xgb))

bar1 = plt.bar(index, importance_xgb, bar_width, label='XGBoost Classifier')

plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importance in Classification Models - XGBoost Classifier')
plt.xticks([p + bar_width / 2 for p in index], features_xgb)
plt.legend()

plt.tight_layout

()
plt.show()